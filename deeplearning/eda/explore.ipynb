{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fa0393d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "v1               0\n",
      "v2               0\n",
      "Unnamed: 2    5522\n",
      "Unnamed: 3    5560\n",
      "Unnamed: 4    5566\n",
      "dtype: int64\n",
      "(5572, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "data = pd.read_csv('../data/spam.csv',encoding='cp437')\n",
    "print(data.head())\n",
    "print(data.isna().sum())\n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe61b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de label : 5574\n",
      "Nombre de text : 5574\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "text = []\n",
    "\n",
    "with open('../data/spam.csv','r') as f :\n",
    "    lines = f.readlines()\n",
    "    for i in lines[1:] :\n",
    "        label = i.split(\",\")[0]\n",
    "        labels.append(1 if label == \"spam\" else 0)\n",
    "        if i.split(',')[1].startswith('\"'):\n",
    "            text.append(i.split('\"')[1])\n",
    "        else :\n",
    "            text.append(i.split(',')[1])\n",
    "\n",
    "print(f\"Nombre de label : {len(labels)}\")\n",
    "print(f\"Nombre de text : {len(text)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "89751a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100277\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "train_tokens = [tokenizer.encode(text) for text in text]\n",
    "print(tokenizer.n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f3e8a1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(22.21707929673484)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lens = [len(seq) for seq in train_tokens]\n",
    "np.mean(seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4fead0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    }
   ],
   "source": [
    "def pad_sequences(sequences, max_length=30):\n",
    "    return [seq[:max_length] + [0] * (max_length - len(seq)) for seq in sequences]\n",
    "\n",
    "train_tokens_seq = pad_sequences(train_tokens)\n",
    "print(len(train_tokens_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5a0346ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.tensor(texts, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "    \n",
    "df_dataset = IMDBDataset(train_tokens_seq, labels)\n",
    "\n",
    "train_size = int(0.8 * len(df_dataset))\n",
    "val_size = len(df_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(df_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9e34780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5809a8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1671,   316,   499, 30315,   369, 13985,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 5159, 20985,     0,   358,  1390,   499,   311,  1935,   220,    17,\n",
      "           477,   220,    18,  9364,   315,  6261,  3432,   304, 10107,  3177,\n",
      "           389,   701,  2849,  4641,     0, 27508,  3177,     0,     0,     0],\n",
      "        [16384,  1791,  1463,   374,   272,   539,   274,    13,  1472,  1288,\n",
      "           617,  3309,   757,  6931,    13,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [19701,    11,  4024,   311,  4950,  4216,    11,  3814,  9471,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    9, 15148,  7422,   922, 43560,   258,  4433,  2579,  6307,   308,\n",
      "          3776,  5542,  5079,   220,    17,  3665,  6920,   258,  1124, 80980,\n",
      "           389,  5542,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   40,  1541,   956,  1440,   577,   323,   577,  1541,   956,  1440,\n",
      "           757,    13, 11244,  6969,   835,   311,   220, 22455,  2421,  1457,\n",
      "           323,  1095,   596,  1505,  1855,  1023,     0,  8442,   220,  3965],\n",
      "        [19182,   499,   649,  2343,    13,  3161, 16498,   409,    13,  8442,\n",
      "           220,   612,  4937,    26,     2,     5,  5289,    26,   662,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [24539, 30125,   690,   384,  9899,   293, 80116,   389,  7160,    30,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 2127,  9250,  3463,   555,   264,  5906,  1263, 59125,   664,  1448,\n",
      "           303,    25,   358,  1168,    86,   577, 12491,   757, 20164,   279,\n",
      "          1938, 38457,   577,  3358,  1168,    86,   279,  8206,   577,  3358],\n",
      "        [17940,   832, 19151,   369,   757,  3067,   497, 31121, 29589,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [18788, 84252,  3221,   653,   344,    13,  2650,   922,   499,    30,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [26833,   374,   810,  7452,  1109, 11326,  1131,   426,  1030,    89,\n",
      "         30169, 33992, 18228,   612,  1141,    26,  1243, 74198,  7151,    11,\n",
      "          2030,  9601,  1176, 74198, 33410,   612,  1141,    26,  1243, 33992],\n",
      "        [   35,   799,   737,   912,  5129,   264, 63414,  1634,    13,  2417,\n",
      "           459, 15715, 41321,  1457,    13,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   36,  1247,   430,   596,   264,  2763,   315,   892,  5423,  2533,\n",
      "          3778, 41097,   374,  1093,   220,    23,  4520,  1317,    13,   358,\n",
      "           649,   956,  3009, 26139,   433,    13,     0,     0,     0,     0],\n",
      "        [ 5618, 26836,   220, 27311,  8874, 21360,  4643,  7214,   439,  1070,\n",
      "           374,   459, 34771,  1984,  8748,   369,   499,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [10149,   433,  1520,   422,   584, 30714,  2133,  1203,  1578, 16986,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   38,   664, 13080,    23, 25237,   497,  3306,    79,  1664,   497,\n",
      "         23609,  2512,   497,  2332,    83, 19226,   497, 40220,  1494,   497,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 2170, 13783,   497,  5038,  7060,    11,  6380,   612,  1141,    26,\n",
      "          3815,  1664,   497, 29589,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   40, 15890,   617,   430,  1790,  2217,   304,   538,    13,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1539,    38,  1863,     0,  1226,   527,  4560,   311,  3729,   549,\n",
      "            13, 59683,   954,  4128,  5039,   430,   499,   617,  2834,   264,\n",
      "         13376, 22386,  4728, 22643, 46372,   946, 94418,  1507,    13,  7290],\n",
      "        [81122,  5509,    11,   602,  3358,  4822,   369,  1093,   459,  6596,\n",
      "          8119,   856,  8071,   374,  2216, 36366,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [19841,  3835,  3067, 29589,    77, 21575, 18912,    13,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [24220,  1314,     0,   358,  3358,  4546,   856, 17401,  6767,  2698,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 4438,    89,  6784,    30, 61893,   577,   436,  7060,   497,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   47, 13634, 11096,   220,    19,    86, 14645,  3432, 49135,  2751,\n",
      "          2683,   497,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   40,  1097,  2133,   311,  6212,    13,   358,  1097, 19781,   315,\n",
      "          5944,    13,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [37830, 23128,    30, 12021,  2751,  4183,  3445,  1202,  1695,   497,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [81710,     0,  4718,   220,  1049,    19,  8785, 22504,   369,   220,\n",
      "         24850, 21962,   431,    22,  5039,   220, 25251, 38223,   451, 16809,\n",
      "         29298, 21387,    13,  2057,  3802,  1650,   220, 27311,  7529, 13135],\n",
      "        [19701,   430,  3952,   779,  1317,    11,  8019,    86,  1457,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 6219,   539,  3318,   719,  1097,   709,   311,  6548,   304, 19675,\n",
      "           779,   690,  1495,   577,  3010,   994,   264,  2766,   810,  1949,\n",
      "           369,  6369,  1131,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1627,   339,  3083, 88160,    56,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 4728,    22,   220,   914,    79,   220,    19, 66868,   648, 17781,\n",
      "           596, 15394,   304, 14998,  5609,   389,  4433, 12881,    13, 25672,\n",
      "          4433,   296,    23,    82,    13, 80204,   350,  5338, 24954,  3414]])\n"
     ]
    }
   ],
   "source": [
    "text, label = next(iter(train_loader))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "00e959f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size,\n",
    "                               embedding_dim=5, \n",
    "                               padding_idx=0)\n",
    "# We create a random list of three integers and use it as input for the embedding\n",
    "# layer and take a look at the output.\n",
    "sample_input = torch.tensor([[1, 2, 3]])\n",
    "embedded_output = embedding_layer(sample_input)\n",
    "print(embedded_output.shape)  # Output: (3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "76c31ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100277, 5])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "67d29c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        pooled = self.pooling(embedded.permute(0, 2, 1)).squeeze(2)\n",
    "        return torch.sigmoid(self.fc(pooled))\n",
    "\n",
    "model = TextClassifier(vocab_size=vocab_size,\n",
    "                      embed_dim=16, \n",
    "                      num_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "29f996ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8810b8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextClassifier(\n",
      "  (embedding): Embedding(100277, 16, padding_idx=0)\n",
      "  (pooling): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TextClassifier                           [1, 1]                    --\n",
       "├─Embedding: 1-1                         [1, 3, 16]                1,604,432\n",
       "├─AdaptiveAvgPool1d: 1-2                 [1, 16, 1]                --\n",
       "├─Linear: 1-3                            [1, 1]                    17\n",
       "==========================================================================================\n",
       "Total params: 1,604,449\n",
       "Trainable params: 1,604,449\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 1.60\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 6.42\n",
       "Estimated Total Size (MB): 6.42\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "print(model)\n",
    "\n",
    "# Print model summary\n",
    "summary(model, input_data=sample_input)  # (batch_size, input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "00443352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6826, Acc: 0.5355, Val Loss: 0.6269, Val Acc: 0.8188\n",
      "Epoch [2/10], Loss: 0.5570, Acc: 0.8975, Val Loss: 0.4980, Val Acc: 0.8942\n",
      "Epoch [3/10], Loss: 0.4279, Acc: 0.9229, Val Loss: 0.3863, Val Acc: 0.9067\n",
      "Epoch [4/10], Loss: 0.3274, Acc: 0.9368, Val Loss: 0.3080, Val Acc: 0.9211\n",
      "Epoch [5/10], Loss: 0.2568, Acc: 0.9491, Val Loss: 0.2546, Val Acc: 0.9372\n",
      "Epoch [6/10], Loss: 0.2087, Acc: 0.9623, Val Loss: 0.2163, Val Acc: 0.9462\n",
      "Epoch [7/10], Loss: 0.1733, Acc: 0.9708, Val Loss: 0.1883, Val Acc: 0.9552\n",
      "Epoch [8/10], Loss: 0.1468, Acc: 0.9767, Val Loss: 0.1669, Val Acc: 0.9614\n",
      "Epoch [9/10], Loss: 0.1266, Acc: 0.9818, Val Loss: 0.1508, Val Acc: 0.9650\n",
      "Epoch [10/10], Loss: 0.1105, Acc: 0.9843, Val Loss: 0.1378, Val Acc: 0.9713\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, epochs=100):\n",
    "    \"\"\"\n",
    "    Function to train a PyTorch model with training and validation datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    model: The neural network model to train.\n",
    "    train_loader: DataLoader for the training dataset.\n",
    "    val_loader: DataLoader for the validation dataset.\n",
    "    criterion: Loss function (e.g., Binary Cross Entropy for classification).\n",
    "    optimizer: Optimization algorithm (e.g., Adam, SGD).\n",
    "    epochs: Number of training epochs (default=100).\n",
    "    \n",
    "    Returns:\n",
    "    history: Dictionary containing loss and accuracy for both training and validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dictionary to store training & validation loss and accuracy over epochs\n",
    "    history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
    "    \n",
    "    for epoch in range(epochs):  # Loop over the number of epochs\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss, correct = 0, 0  # Initialize total loss and correct predictions\n",
    "        \n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()  # Reset gradients before each batch\n",
    "            outputs = model(inputs).squeeze()  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation (compute gradients)\n",
    "            optimizer.step()  # Update model parameters\n",
    "            \n",
    "            total_loss += loss.item()  # Accumulate batch loss\n",
    "            correct += ((outputs > 0.5) == labels).sum().item()  # Count correct predictions\n",
    "        \n",
    "        # Compute average loss and accuracy for training\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation phase (without gradient computation)\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss, val_correct = 0, 0\n",
    "        with torch.no_grad():  # No need to compute gradients during validation\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs).squeeze()  # Forward pass\n",
    "                loss = criterion(outputs, labels)  # Compute loss\n",
    "                val_loss += loss.item()  # Accumulate validation loss\n",
    "                val_correct += ((outputs > 0.5) == labels).sum().item()  # Count correct predictions\n",
    "        \n",
    "        # Compute average loss and accuracy for validation\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "        \n",
    "        # Store metrics in history dictionary\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['accuracy'].append(train_acc)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "        \n",
    "        # Print training progress\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return history  # Return training history\n",
    "\n",
    "history = train(model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae29116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_here (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
